{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Translation and Sentiment Analysis using Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Overview:\n",
    "\n",
    "The objective of this project is to analyze the sentiment of movie reviews in three different languages - English, French, and Spanish. We have been given 30 movies, 10 in each language, along with their reviews and synopses in separate CSV files named `movie_reviews_eng.csv`, `movie_reviews_fr.csv`, and `movie_reviews_sp.csv`.\n",
    "\n",
    "- The first step of this project is to convert the French and Spanish reviews and synopses into English. This will allow us to analyze the sentiment of all reviews in the same language. We will be using pre-trained transformers from HuggingFace to achieve this task.\n",
    "\n",
    "- Once the translations are complete, we will create a single dataframe that contains all the movies along with their reviews, synopses, and year of release in all three languages. This dataframe will be used to perform sentiment analysis on the reviews of each movie.\n",
    "\n",
    "- Finally, we will use pretrained transformers from HuggingFace to analyze the sentiment of each review. The sentiment analysis results will be added to the dataframe. The final dataframe will have 30 rows\n",
    "\n",
    "\n",
    "The output of the project will be a CSV file with a header row that includes column names such as **Title**, **Year**, **Synopsis**, **Review**, **Review Sentiment**, and **Original Language**. The **Original Language** column will indicate the language of the review and synopsis (*en/fr/sp*) before translation. The dataframe will consist of 30 rows, with each row corresponding to a movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in c:\\users\\brend\\.conda\\envs\\udacity\\lib\\site-packages (0.10.1)\n",
      "Collecting huggingface_hub\n",
      "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "     -------------------------------------- 268.8/268.8 kB 1.5 MB/s eta 0:00:00\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2023.1.0-py3-none-any.whl (143 kB)\n",
      "     -------------------------------------- 143.0/143.0 kB 2.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\brend\\.conda\\envs\\udacity\\lib\\site-packages (from huggingface_hub) (4.4.0)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\brend\\.conda\\envs\\udacity\\lib\\site-packages (from huggingface_hub) (6.7.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\brend\\.conda\\envs\\udacity\\lib\\site-packages (from huggingface_hub) (3.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\brend\\.conda\\envs\\udacity\\lib\\site-packages (from huggingface_hub) (24.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\brend\\.conda\\envs\\udacity\\lib\\site-packages (from huggingface_hub) (4.64.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\brend\\.conda\\envs\\udacity\\lib\\site-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\brend\\.conda\\envs\\udacity\\lib\\site-packages (from huggingface_hub) (2.28.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\brend\\.conda\\envs\\udacity\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\brend\\.conda\\envs\\udacity\\lib\\site-packages (from importlib-metadata->huggingface_hub) (3.15.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\brend\\.conda\\envs\\udacity\\lib\\site-packages (from requests->huggingface_hub) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\brend\\.conda\\envs\\udacity\\lib\\site-packages (from requests->huggingface_hub) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\brend\\.conda\\envs\\udacity\\lib\\site-packages (from requests->huggingface_hub) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\brend\\.conda\\envs\\udacity\\lib\\site-packages (from requests->huggingface_hub) (1.26.14)\n",
      "Installing collected packages: fsspec, huggingface_hub\n",
      "  Attempting uninstall: huggingface_hub\n",
      "    Found existing installation: huggingface-hub 0.10.1\n",
      "    Uninstalling huggingface-hub-0.10.1:\n",
      "      Successfully uninstalled huggingface-hub-0.10.1\n",
      "Successfully installed fsspec-2023.1.0 huggingface_hub-0.16.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data from `.csv` files and then preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use the `pd.read_csv()` function to read the movie_review_*.csv files into 3 separate pandas dataframes\n",
    "\n",
    "# Note: All the dataframes would have different column names. For testing purposes\n",
    "# you should have the following column names/headers -> [Title, Year, Synopsis, Review]\n",
    "\n",
    "def preprocess_data() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads movie data from .csv files, map column names, add the \"Original Language\" column,\n",
    "    and finally concatenate in one resultant dataframe called \"df\".\n",
    "    \"\"\"\n",
    "    df_eng = pd.read_csv(\"data/movie_reviews_eng.csv\")\n",
    "    #TODO 1: Add your code here\n",
    "    df_eng[\"Original Language\"] = \"English\"\n",
    "    df_fr = pd.read_csv(\"data/movie_reviews_fr.csv\")\n",
    "    df_fr[\"Original Language\"] = \"French\"\n",
    "    df_sp = pd.read_csv(\"data/movie_reviews_sp.csv\")\n",
    "    df_sp[\"Original Language\"] = \"Spanish\"\n",
    "    \n",
    "    df_fr.columns = df_eng.columns\n",
    "    df_sp.columns = df_eng.columns\n",
    "    df = pd.concat([df_eng, df_fr, df_sp]).reset_index(drop='True')\n",
    "    df.rename(columns={'Movie / TV Series': 'Title'}, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Year</th>\n",
       "      <th>Synopsis</th>\n",
       "      <th>Review</th>\n",
       "      <th>Original Language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>La La Land</td>\n",
       "      <td>2016</td>\n",
       "      <td>Cette comédie musicale raconte l'histoire d'un...</td>\n",
       "      <td>\"La La Land est un film absolument magnifique ...</td>\n",
       "      <td>French</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>El Bar</td>\n",
       "      <td>2017</td>\n",
       "      <td>Un grupo de personas quedan atrapadas en un ba...</td>\n",
       "      <td>\"El Bar es una película ridícula y sin sentido...</td>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Torrente: El brazo tonto de la ley</td>\n",
       "      <td>1998</td>\n",
       "      <td>En esta comedia española, un policía corrupto ...</td>\n",
       "      <td>\"Torrente es una película vulgar y ofensiva qu...</td>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Forrest Gump</td>\n",
       "      <td>1994</td>\n",
       "      <td>Forrest Gump (Tom Hanks) is a simple man with ...</td>\n",
       "      <td>\"Forrest Gump is a heartwarming and inspiratio...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Le Fabuleux Destin d'Amélie Poulain</td>\n",
       "      <td>2001</td>\n",
       "      <td>Cette comédie romantique raconte l'histoire d'...</td>\n",
       "      <td>\"Le Fabuleux Destin d'Amélie Poulain est un fi...</td>\n",
       "      <td>French</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>El Incidente</td>\n",
       "      <td>2014</td>\n",
       "      <td>En esta película de terror mexicana, un grupo ...</td>\n",
       "      <td>\"El Incidente es una película aburrida y sin s...</td>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Nice Guys</td>\n",
       "      <td>2016</td>\n",
       "      <td>In 1970s Los Angeles, a private eye (Ryan Gosl...</td>\n",
       "      <td>\"The Nice Guys tries too hard to be funny, and...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Solo: A Star Wars Story</td>\n",
       "      <td>2018</td>\n",
       "      <td>A young Han Solo (Alden Ehrenreich) joins a gr...</td>\n",
       "      <td>\"Dull and pointless, with none of the magic of...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Roma</td>\n",
       "      <td>2018</td>\n",
       "      <td>Cleo (Yalitza Aparicio) es una joven empleada ...</td>\n",
       "      <td>\"Roma es una película hermosa y conmovedora qu...</td>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Inception</td>\n",
       "      <td>2010</td>\n",
       "      <td>Dom Cobb (Leonardo DiCaprio) is a skilled thie...</td>\n",
       "      <td>\"Inception is a mind-bending and visually stun...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Title  Year  \\\n",
       "10                           La La Land  2016   \n",
       "27                               El Bar  2017   \n",
       "28   Torrente: El brazo tonto de la ley  1998   \n",
       "2                          Forrest Gump  1994   \n",
       "14  Le Fabuleux Destin d'Amélie Poulain  2001   \n",
       "29                         El Incidente  2014   \n",
       "7                         The Nice Guys  2016   \n",
       "8               Solo: A Star Wars Story  2018   \n",
       "20                                 Roma  2018   \n",
       "4                             Inception  2010   \n",
       "\n",
       "                                             Synopsis  \\\n",
       "10  Cette comédie musicale raconte l'histoire d'un...   \n",
       "27  Un grupo de personas quedan atrapadas en un ba...   \n",
       "28  En esta comedia española, un policía corrupto ...   \n",
       "2   Forrest Gump (Tom Hanks) is a simple man with ...   \n",
       "14  Cette comédie romantique raconte l'histoire d'...   \n",
       "29  En esta película de terror mexicana, un grupo ...   \n",
       "7   In 1970s Los Angeles, a private eye (Ryan Gosl...   \n",
       "8   A young Han Solo (Alden Ehrenreich) joins a gr...   \n",
       "20  Cleo (Yalitza Aparicio) es una joven empleada ...   \n",
       "4   Dom Cobb (Leonardo DiCaprio) is a skilled thie...   \n",
       "\n",
       "                                               Review Original Language  \n",
       "10  \"La La Land est un film absolument magnifique ...            French  \n",
       "27  \"El Bar es una película ridícula y sin sentido...           Spanish  \n",
       "28  \"Torrente es una película vulgar y ofensiva qu...           Spanish  \n",
       "2   \"Forrest Gump is a heartwarming and inspiratio...           English  \n",
       "14  \"Le Fabuleux Destin d'Amélie Poulain est un fi...            French  \n",
       "29  \"El Incidente es una película aburrida y sin s...           Spanish  \n",
       "7   \"The Nice Guys tries too hard to be funny, and...           English  \n",
       "8   \"Dull and pointless, with none of the magic of...           English  \n",
       "20  \"Roma es una película hermosa y conmovedora qu...           Spanish  \n",
       "4   \"Inception is a mind-bending and visually stun...           English  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text translation\n",
    "\n",
    "Translate the **Review** and **Synopsis** column values to English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "HFValidationError",
     "evalue": "Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: 'MarianMTModel(\n  (model): MarianModel(\n    (shared): Embedding(59514, 512, padding_idx=59513)\n    (encoder): MarianEncoder(\n      (embed_tokens): Embedding(59514, 512, padding_idx=59513)\n      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n      (layers): ModuleList(\n        (0): MarianEncoderLayer(\n          (self_attn): MarianAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): SiLUActivation()\n          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (1): MarianEncoderLayer(\n          (self_attn): MarianAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): SiLUActivation()\n          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (2): MarianEncoderLayer(\n          (self_attn): MarianAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): SiLUActivation()\n          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (3): MarianEncoderLayer(\n          (self_attn): MarianAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): SiLUActivation()\n          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (4): MarianEncoderLayer(\n          (self_attn): MarianAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): SiLUActivation()\n          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (5): MarianEncoderLayer(\n          (self_attn): MarianAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): SiLUActivation()\n          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n    (decoder): MarianDecoder(\n      (embed_tokens): Embedding(59514, 512, padding_idx=59513)\n      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n      (layers): ModuleList(\n        (0): MarianDecoderLayer(\n          (self_attn): MarianAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (activation_fn): SiLUActivation()\n          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): MarianAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (1): MarianDecoderLayer(\n          (self_attn): MarianAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (activation_fn): SiLUActivation()\n          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): MarianAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (2): MarianDecoderLayer(\n          (self_attn): MarianAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (activation_fn): SiLUActivation()\n          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): MarianAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (3): MarianDecoderLayer(\n          (self_attn): MarianAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (activation_fn): SiLUActivation()\n          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): MarianAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (4): MarianDecoderLayer(\n          (self_attn): MarianAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (activation_fn): SiLUActivation()\n          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): MarianAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (5): MarianDecoderLayer(\n          (self_attn): MarianAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (activation_fn): SiLUActivation()\n          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): MarianAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (lm_head): Linear(in_features=512, out_features=59514, bias=False)\n)'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_36776\\2052904671.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mfr_en_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMarianMTModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfr_en_model_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mes_en_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMarianMTModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mes_en_model_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mfr_en_tokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMarianTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfr_en_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mes_en_tokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMarianTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mes_en_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\udacity\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1748\u001b[0m                     \u001b[0m_raise_exceptions_for_missing_entries\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1749\u001b[0m                     \u001b[0m_raise_exceptions_for_connection_errors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1750\u001b[1;33m                     \u001b[0m_commit_hash\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcommit_hash\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1751\u001b[0m                 )\n\u001b[0;32m   1752\u001b[0m                 \u001b[0mcommit_hash\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_commit_hash\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresolved_vocab_files\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfile_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcommit_hash\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\udacity\\lib\\site-packages\\transformers\\utils\\hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[0;32m    418\u001b[0m             \u001b[0mresume_download\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_download\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m             \u001b[0muse_auth_token\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 420\u001b[1;33m             \u001b[0mlocal_files_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    421\u001b[0m         )\n\u001b[0;32m    422\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\udacity\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    108\u001b[0m         ):\n\u001b[0;32m    109\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"repo_id\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"from_id\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"to_id\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m                 \u001b[0mvalidate_repo_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"token\"\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0marg_value\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\udacity\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py\u001b[0m in \u001b[0;36mvalidate_repo_id\u001b[1;34m(repo_id)\u001b[0m\n\u001b[0;32m    163\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mREPO_ID_REGEX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrepo_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m         raise HFValidationError(\n\u001b[1;32m--> 165\u001b[1;33m             \u001b[1;34m\"Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m             \u001b[1;34m\" forbidden, '-' and '.' cannot start or end the name, max length is 96:\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m             \u001b[1;34mf\" '{repo_id}'.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mHFValidationError\u001b[0m: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: 'MarianMTModel(\n  (model): MarianModel(\n    (shared): Embedding(59514, 512, padding_idx=59513)\n    (encoder): MarianEncoder(\n      (embed_tokens): Embedding(59514, 512, padding_idx=59513)\n      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n      (layers): ModuleList(\n        (0): MarianEncoderLayer(\n          (self_attn): MarianAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): SiLUActivation()\n          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (1): MarianEncoderLayer(\n          (self_attn): MarianAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): SiLUActivation()\n          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (2): MarianEncoderLayer(\n          (self_attn): MarianAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): SiLUActivation()\n          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (3): MarianEncoderLayer(\n          (self_attn): MarianAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): SiLUActivation()\n          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (4): MarianEncoderLayer(\n          (self_attn): MarianAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): SiLUActivation()\n          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (5): MarianEncoderLayer(\n          (self_attn): MarianAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): SiLUActivation()\n          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n    (decoder): MarianDecoder(\n      (embed_tokens): Embedding(59514, 512, padding_idx=59513)\n      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n      (layers): ModuleList(\n        (0): MarianDecoderLayer(\n          (self_attn): MarianAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (activation_fn): SiLUActivation()\n          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): MarianAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (1): MarianDecoderLayer(\n          (self_attn): MarianAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (activation_fn): SiLUActivation()\n          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): MarianAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (2): MarianDecoderLayer(\n          (self_attn): MarianAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (activation_fn): SiLUActivation()\n          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): MarianAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (3): MarianDecoderLayer(\n          (self_attn): MarianAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (activation_fn): SiLUActivation()\n          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): MarianAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (4): MarianDecoderLayer(\n          (self_attn): MarianAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (activation_fn): SiLUActivation()\n          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): MarianAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (5): MarianDecoderLayer(\n          (self_attn): MarianAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (activation_fn): SiLUActivation()\n          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): MarianAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (lm_head): Linear(in_features=512, out_features=59514, bias=False)\n)'."
     ]
    }
   ],
   "source": [
    "# load translation models and tokenizers\n",
    "# TODO 2: Update the code below\n",
    "fr_en_model_name = \"Helsinki-NLP/opus-mt-fr-en\"\n",
    "es_en_model_name = \"Helsinki-NLP/opus-mt-es-en\"\n",
    "fr_en_model = MarianMTModel.from_pretrained(fr_en_model_name)\n",
    "es_en_model = MarianMTModel.from_pretrained(es_en_model_name)\n",
    "fr_en_tokenizer = MarianTokenizer.from_pretrained(fr_en_model)\n",
    "es_en_tokenizer = MarianTokenizer.from_pretrained(es_en_model)\n",
    "\n",
    "# TODO 3: Complete the function below\n",
    "def translate(text: str, model, tokenizer) -> str:\n",
    "    \"\"\"\n",
    "    function to translate a text using a model and tokenizer\n",
    "    \"\"\"\n",
    "    # encode the text using the tokenizer\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "\n",
    "    # generate the translation using the model\n",
    "    outputs = model.generate(**inputs)\n",
    "\n",
    "    # decode the generated output and return the translated text\n",
    "    decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 4: Update the code below\n",
    "\n",
    "# filter reviews in French and translate to English\n",
    "fr_reviews = None\n",
    "fr_reviews_en = None\n",
    "\n",
    "# filter synopsis in French and translate to English\n",
    "fr_synopsis = None\n",
    "fr_synopsis_en = None\n",
    "\n",
    "# filter reviews in Spanish and translate to English\n",
    "es_reviews = None\n",
    "es_reviews_en = None\n",
    "\n",
    "# filter synopsis in Spanish and translate to English\n",
    "es_synopsis = None\n",
    "es_synopsis_en = None\n",
    "\n",
    "# update dataframe with translated text\n",
    "# add the translated reviews and synopsis - you can overwrite the existing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis\n",
    "\n",
    "Use HuggingFace pretrained model for sentiment analysis of the reviews. Store the sentiment result **Positive** or **Negative** in a new column titled **Sentiment** in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 5: Update the code below\n",
    "# load sentiment analysis model\n",
    "model_name = None\n",
    "sentiment_classifier = None\n",
    "\n",
    "# TODO 6: Complete the function below\n",
    "def analyze_sentiment(text, classifier):\n",
    "    \"\"\"\n",
    "    function to perform sentiment analysis on a text using a model\n",
    "    \"\"\"\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 7: Add code below for sentiment analysis\n",
    "# perform sentiment analysis on reviews and store results in new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the results to a .csv file\n",
    "df.to_csv(\"result/reviews_with_sentiment.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
